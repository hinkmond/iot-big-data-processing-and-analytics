My final project was to use the JSON format of HealthCheckResponse of 
Smart Home Skill API of amazon. The reference to the JSON used is HealthCheckResponse
https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/smart-home-skill-api-reference
Health Check requests are periodically sent by the Smart Home Skill API to the skill adapter which 
will be the home device
The JSON response format of HealthCheckResponse is what I have used for the project

The messageId is generated similar to the guid from the iotsimulator.py
I have modified the iotsimulator.py to generate JSON messages that are in the form HealthCheck Response
Passed HealthCheckResponse example:
{
        "header": {
            "messageId": "f9905dc8-b861-4912-bcf7-5b90f62b3a71",
            "name": "HealthCheckResponse",
            "namespace": "Alexa.ConnectedHome.System",
            "payloadVersion": "2"
        },
        "payload": {
            "description": "The system is currently healthy",
            "isHealthy": "true"
        }
}
Failed HealthCheckResponse example:
{
        "header": {
            "messageId": "f9905dc8-b861-4912-bcf7-5b90f62b3a71",
            "name": "HealthCheckResponse",
            "namespace": "Alexa.ConnectedHome.System",
            "payloadVersion": "2"
        },
        "payload": {
            "description": "The system is currently not healthy",
            "isHealthy": "false"
        }
    }

The program final-direct-iot-sql.py simulates sucess and failed HealthCheckResponse response JSON, 
the type of message generated is based on a random value. final_input.json is an example generated by running
final_iotsimulator.py with a parameter of 100.

For Spark Processing the output of iotsimulator is sent to Kafka queue with command
./final_iotsimulator.py 100 | ./kafka_2.11-0.10.1.0/bin/kafka-console-producer.sh \
 --broker-list localhost:9092 --topic iotmsgs

The messages from the Kafka queue is processed by Spark Streaming. 
spark-submit --jars spark-streaming-kafka-0-8-assembly_2.11-2.0.0-preview.jar \
 ./final-direct-iot-sql.py localhost:9092 iotmsgs 2> /tmp/big_data_str_sql.log

The python script final-direct-iot-sql.py first generates an RDD from JSON. Then SparkSQL registers the 
JSON SQL Context as a temporary SQL table. A Data Frame is generated from the SQL Table with rows 
that have isHealthy=false. The print function generates error message in along with the time for particular 
row of data frame. In real life this would be email, text alert based on the user's configured preference 
for the Alexa device.

One of the problems in the processing part of my code is that all the white space is trimmed, so the input json
payload.description has white space trimmed. Due to the this, did not use the payload.description in the 
alert message print function. One of the earlier problems is generated JSON was incorrect, so was giving error
of corrupt data

In real world applications the likely usage will be that HealthCheckResponses generated from multiple Alexa devices 
will go to Amazon Services and the processing of the alert will be done according 
to the alert method (email, text, phone) and preferences registered for the corresponding device.

